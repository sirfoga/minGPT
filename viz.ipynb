{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appointed-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coordinate-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from mingpt.utils import set_seed, sample\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ordered-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # make deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './best_model.pt'\n",
    "data_path = './data/brain.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nasty-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_S = dict(\n",
    "    embd_pdrop=0.0,\n",
    "    resid_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    n_layer=24,\n",
    "    n_head=8,\n",
    "    n_embd=512,\n",
    ")\n",
    "\n",
    "\n",
    "def now_utc():  # unix time\n",
    "    seconds = round(time.time())\n",
    "    millis = seconds * 1000\n",
    "    unix = int(millis)\n",
    "    return unix\n",
    "\n",
    "\n",
    "def load_pickle(f_path):\n",
    "    with open(f_path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "\n",
    "def get_model(mconf):\n",
    "    return GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "approved-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_GPT = dict(\n",
    "    n_layer=16,\n",
    "    n_embd=256\n",
    ")\n",
    "MY_GPT = {**GPT_S, **MY_GPT}  # inherit all other params\n",
    "\n",
    "    \n",
    "mconf = GPTConfig(\n",
    "    256,\n",
    "    1023,\n",
    "    **MY_GPT,\n",
    "    bert=False,\n",
    "    use_embd=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "original-digit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = get_model(mconf)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "checked-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(X, y, test_size, random_state=42, verbose=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state  # reproducible results\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        logging.getLogger(__name__).info('train data: X ~ {}, y ~ {}'.format(X_train.shape, y_train.shape))\n",
    "        logging.getLogger(__name__).info('test data: X ~ {}, y ~ {}'.format(X_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def get_data(file_path, max_imgs=2000):\n",
    "    dataset = load_pickle(Path(file_path).expanduser())\n",
    "\n",
    "    if len(dataset) == 2:  # (images, masks)\n",
    "        X = dataset[0]  # list of images\n",
    "        y = dataset[1]  # list of corresponding mask\n",
    "    else:  # unsupervised list of images\n",
    "        X = np.array(dataset, dtype='float32')[:max_imgs]\n",
    "        y = np.zeros(len(X))\n",
    "\n",
    "    pixel_size = X.shape[1]  # should be == X.shape[2] == 32\n",
    "    X = np.array(np.ceil(X * 255), dtype='float32')  # convert pixels to [0, 255] range\n",
    "    y = np.array(np.ceil(y * 255), dtype='float32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(X, y, 0.3, verbose=True)\n",
    "\n",
    "    tensor_X_train = torch.Tensor(X_train)  # tensors\n",
    "    tensor_y_train = torch.Tensor(y_train)\n",
    "    tensor_X_test = torch.Tensor(X_test)\n",
    "    tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "    t_train_dataset = TensorDataset(tensor_X_train, tensor_y_train)\n",
    "    t_test_dataset = TensorDataset(tensor_X_test, tensor_y_test)\n",
    "\n",
    "    return t_train_dataset, t_test_dataset, X_train\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, pt_dataset, perm=None):\n",
    "        self.pt_dataset = pt_dataset\n",
    "\n",
    "        flattened_image_size = 32 * 32\n",
    "        self.perm = torch.arange(flattened_image_size) if perm is None else perm\n",
    "\n",
    "        self.vocab_size = 256  # possible values for pixels\n",
    "        self.block_size = flattened_image_size - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pt_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_channels = 1  # grayscale\n",
    "\n",
    "        x, y = self.pt_dataset[idx]\n",
    "        x = torch.from_numpy(np.array(x)).view(-1, image_channels)  # flatten out all pixels\n",
    "        x = x[self.perm].float()  # reshuffle pixels with any fixed permutation and -> float\n",
    "        a = x[:, 0]\n",
    "        return a[:-1], a[1:]  # always just predict the next one in the sequence\n",
    "    \n",
    "    \n",
    "def sample_some(trainer, model, dataset, X_train, n_samples=40, out_path='./samples.png'):\n",
    "    prob = model_first_token(dataset, X_train)\n",
    "\n",
    "    start_pixel = np.random.choice(np.arange(dataset.vocab_size), size=(n_samples, 1), replace=True, p=prob.numpy())\n",
    "    start_pixel = torch.from_numpy(start_pixel).to(trainer.device)\n",
    "    flattened_image_size = 32 * 32\n",
    "    pixels = sample(model, start_pixel, flattened_image_size - 1, temperature=1.0, sample=True, top_k=40)\n",
    "\n",
    "    # for visualization we have to invert the permutation used to produce the pixels\n",
    "    iperm = torch.argsort(dataset.perm)\n",
    "\n",
    "    pixel_size = 32\n",
    "\n",
    "    n_cols = 8\n",
    "    n_rows = n_samples // n_cols\n",
    "    fig, axis = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "    for i, ax in enumerate(axis.ravel()):\n",
    "        pxi = pixels[i][iperm]  # undo the encoding permutation\n",
    "        pxi = pxi.view(pixel_size, pixel_size).cpu().numpy().astype(np.uint8)  # grayscale -> 2D\n",
    "\n",
    "        ax.imshow(pxi, cmap='magma')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(out_path)\n",
    "    \n",
    "    \n",
    "def train(model, n_epochs, train_dataset, test_dataset, checkpoint_path):\n",
    "    tokens_per_epoch = len(train_dataset) * train_dataset.block_size\n",
    "\n",
    "    # initialize a trainer instance and kick off training\n",
    "    tconf = TrainerConfig(\n",
    "        max_epochs=n_epochs,\n",
    "        batch_size=4,\n",
    "        learning_rate=3e-3,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=0,\n",
    "        lr_decay=True,\n",
    "        warmup_tokens=tokens_per_epoch,\n",
    "        final_tokens=n_epochs * tokens_per_epoch,\n",
    "        ckpt_path=checkpoint_path,\n",
    "        num_workers=1\n",
    "    )\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    # already train trainer.train()\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def model_first_token(dataset, X_train, n_clusters=256):\n",
    "    counts = torch.ones(n_clusters)  # start counts as 1 not zero, this is called \"smoothing\"\n",
    "    rp = torch.randperm(len(dataset))\n",
    "    nest = X_train.shape[0] // 2  # how many images to use for the estimation\n",
    "\n",
    "    for i in range(nest):\n",
    "        a, _ = dataset[int(rp[i])]\n",
    "        t = a[0].item()  # index of first token in the sequence\n",
    "        counts[int(t)] += 1\n",
    "\n",
    "    prob = counts / counts.sum()  # normalize to have sum (prob) = 1\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continued-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_dataset, t_test_dataset, X_train = get_data(data_path)  # raw data\n",
    "train_dataset = ImageDataset(t_train_dataset)  # build dataset\n",
    "test_dataset = ImageDataset(t_test_dataset)\n",
    "\n",
    "trainer = train(model, 50, train_dataset, test_dataset, './latest_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "floating-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 10 2\n"
     ]
    }
   ],
   "source": [
    "b, t, d = token_embeddings.size()\n",
    "print(b, t, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alleged-kentucky",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_embeddings = torch.rand(4, 10, 2)\n",
    "_b, _t, _d = token_embeddings.size()\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = 1\n",
    "fig, axis = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "for i, ax in enumerate(axis.ravel()):\n",
    "    ti = token_embeddings[i]\n",
    "    ti = ti.view(_t, _d).numpy()\n",
    "\n",
    "    ax.imshow(ti, cmap='jet')\n",
    "    # ax.axis('off')\n",
    "    \n",
    "    # ax.set_xlim(0, 1)\n",
    "    # ax.set_ylim(0, 10)\n",
    "    ax.set_aspect(aspect=2.0/10.0)\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "# plt.savefig('wowee.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
