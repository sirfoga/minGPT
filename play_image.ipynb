{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "  format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "  datefmt=\"%Y-%d-%d %H:%M:%S\",\n",
    "  level=logging.INFO,\n",
    "  filename='play_image.log',\n",
    ")\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.utils import set_seed, sample\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "set_seed(42)  # make deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f_path):\n",
    "    with open(f_path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "\n",
    "def get_train_test_split(X, y, test_size, random_state=42, verbose=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state  # reproducible results\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print('train data: X ~ {}, y ~ {}'.format(X_train.shape, y_train.shape))\n",
    "        print('test data: X ~ {}, y ~ {}'.format(X_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    dataset = load_pickle(Path(file_path).expanduser())  # list of (image, mask)\n",
    "    X = dataset[0]  # list of images\n",
    "    y = dataset[1]  # list of corresponding mask\n",
    "\n",
    "    pixel_size = X.shape[1]  # should be = X.shape[2] = 32\n",
    "    image_channels = X.shape[-1]  # should be = 1\n",
    "\n",
    "    # convert pixels to [0, 255] range\n",
    "    X = np.array(np.ceil(X * 255), dtype='float32')\n",
    "    y = np.array(np.ceil(y * 255), dtype='float32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_train_test_split(X, y, 0.3, verbose=True)\n",
    "    \n",
    "    tensor_X_train = torch.Tensor(X_train)  # tensors\n",
    "    tensor_y_train = torch.Tensor(y_train)\n",
    "    tensor_X_test = torch.Tensor(X_test)\n",
    "    tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "    t_train_dataset = TensorDataset(tensor_X_train, tensor_y_train)\n",
    "    t_test_dataset = TensorDataset(tensor_X_test, tensor_y_test)\n",
    "\n",
    "    return t_train_dataset, t_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x, ncluster, niter=10):\n",
    "    N, D = x.size()\n",
    "    c = x[torch.randperm(N)[:ncluster]]  # init clusters at random\n",
    "    for i in range(niter):\n",
    "        # assign all pixels to the closest codebook element\n",
    "        a = ((x[:, None, :] - c[None, :, :])**2).sum(-1).argmin(1)\n",
    "\n",
    "        # move each codebook element to be the mean of the pixels that assigned to it\n",
    "        c = torch.stack([x[a==k].mean(0) for k in range(ncluster)])\n",
    "\n",
    "        # re-assign any poorly positioned codebook elements\n",
    "        nanix = torch.any(torch.isnan(c), dim=1)\n",
    "        ndead = nanix.sum().item()\n",
    "\n",
    "        print('done step %d/%d, re-initialized %d dead clusters' % (i+1, niter, ndead))\n",
    "        c[nanix] = x[torch.randperm(N)[:ndead]] # re-init dead clusters\n",
    "\n",
    "    return c\n",
    "  \n",
    "\n",
    "def get_quantization(dataset, n_clusters=256, do_plot=False):\n",
    "    # get random 5 pixels per image and stack them all up as rgb values to get half a million random pixels\n",
    "    n_pixels = 5\n",
    "    flattened_image_size = 32 * 32\n",
    "    pluck_rgb = lambda x: torch.from_numpy(np.array(x)).view(flattened_image_size, image_channels)[torch.randperm(flattened_image_size)[:n_pixels], :]\n",
    "    px = torch.cat([pluck_rgb(x) for x, y in dataset], dim=0).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        C = kmeans(px, n_clusters, niter=8)\n",
    "        \n",
    "    if do_plot:  # to visualize how much we've lost in the discretization\n",
    "        n_samples = 32\n",
    "        n_cols = 8\n",
    "        n_rows = n_samples // n_cols\n",
    "        fig, axis = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "        for ax, i in zip(axis.ravel(), np.random.randint(0, len(t_train_dataset), size=n_samples)):\n",
    "            # encode and decode random data\n",
    "            x, y = t_train_dataset[i]\n",
    "            xpt = torch.from_numpy(np.array(x)).float().view(flattened_image_size, image_channels)\n",
    "            ix = ((xpt[:, None, :] - C[None, :, :])**2).sum(-1).argmin(1)  # cluster assignments for each pixel\n",
    "\n",
    "            sample = C[ix].view(pixel_size, pixel_size, image_channels).numpy().astype(np.uint8)\n",
    "            ax.imshow(sample[..., 0], cmap='magma')\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.savefig('results/clustered.png')\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    wrap up the pytorch CIFAR-10 dataset into our own, which will convert images into sequences of integers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pt_dataset, clusters, perm=None):\n",
    "        self.pt_dataset = pt_dataset\n",
    "        self.clusters = clusters\n",
    "        \n",
    "        flattened_image_size = 32 * 32\n",
    "        self.perm = torch.arange(flattened_image_size) if perm is None else perm\n",
    "\n",
    "        self.vocab_size = 256\n",
    "        self.block_size = flattened_image_size - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pt_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.pt_dataset[idx]\n",
    "        x = torch.from_numpy(np.array(x)).view(-1, image_channels)  # flatten out all pixels\n",
    "        x = x[self.perm].float()  # reshuffle pixels with any fixed permutation and -> float\n",
    "        a = ((x[:, None, :] - self.clusters[None, :, :])**2).sum(-1).argmin(1)  # cluster assignments\n",
    "        return a[:-1], a[1:]  # always just predict the next one in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_XS = dict(\n",
    "    embd_pdrop=0.0,\n",
    "    resid_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    n_layer=12,\n",
    "    n_head=8,\n",
    "    n_embd=256  \n",
    ")\n",
    "\n",
    "GPT_S = dict(\n",
    "    embd_pdrop=0.0,\n",
    "    resid_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    n_layer=24,\n",
    "    n_head=8,\n",
    "    n_embd=512   \n",
    ")\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    mconf = GPTConfig(\n",
    "        train_dataset.vocab_size,\n",
    "        train_dataset.block_size,\n",
    "        **GPT_XS,\n",
    "        bert=True,\n",
    "    )\n",
    "    return GPT(mconf)\n",
    "  \n",
    "\n",
    "def train(model, n_epochs, train_dataset, test_dataset, checkpoint_path):\n",
    "    tokens_per_epoch = len(train_dataset) * train_dataset.block_size\n",
    "  \n",
    "    # initialize a trainer instance and kick off training\n",
    "    tconf = TrainerConfig(\n",
    "        max_epochs=n_epochs, \n",
    "        batch_size=4, \n",
    "        learning_rate=3e-3,\n",
    "        betas=(0.9, 0.95), \n",
    "        weight_decay=0,\n",
    "        lr_decay=True,\n",
    "        warmup_tokens=tokens_per_epoch,\n",
    "        final_tokens=train_epochs * tokens_per_epoch,\n",
    "        ckpt_path=checkpoint_path,\n",
    "        num_workers=1\n",
    "    )\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    trainer.train()  # WARNING: this blows CPU\n",
    "\n",
    "    \n",
    "def sample_some(model, n_samples=40, out_path='./results/samples.png'):\n",
    "    # to sample we also have to technically \"train\" a separate model for the first token in the sequence\n",
    "    # we are going to do so below simply by calculating and normalizing the histogram of the first token\n",
    "\n",
    "    counts = torch.ones(ncluster)  # start counts as 1 not zero, this is called \"smoothing\"\n",
    "    rp = torch.randperm(len(train_dataset))\n",
    "    nest = X_train.shape[0] // 2  # how many images to use for the estimation\n",
    "    for i in range(nest):\n",
    "        a, _ = train_dataset[int(rp[i])]\n",
    "        t = a[0].item()  # index of first token in the sequence\n",
    "        counts[t] += 1\n",
    "\n",
    "    prob = counts / counts.sum()  # normalize to have sum (prob) = 1\n",
    "    \n",
    "    start_pixel = np.random.choice(np.arange(C.size(0)), size=(n_samples, 1), replace=True, p=prob.numpy())\n",
    "    start_pixel = torch.from_numpy(start_pixel).to(trainer.device)\n",
    "    pixels = sample(model, start_pixel, flattened_image_size - 1, temperature=1.0, sample=True, top_k=40)  # WARNING: this blows CPU\n",
    "    \n",
    "    # for visualization we have to invert the permutation used to produce the pixels\n",
    "    iperm = torch.argsort(train_dataset.perm)\n",
    "\n",
    "    n_cols = 8\n",
    "    n_rows = n_samples // n_cols\n",
    "    fig, axis = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "    for i, ax in enumerate(axis.ravel()):\n",
    "        pxi = pixels[i][iperm]  # undo the encoding permutation\n",
    "        pxi = C[pxi].view(pixel_size, pixel_size).numpy().astype(np.uint8)  # grayscale -> 2D\n",
    "\n",
    "        ax.imshow(pxi, cmap='magma')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_dataset, t_test_dataset = get_data('~/martin/minGPT_data.pkl')\n",
    "C = get_quantization(t_train_dataset)\n",
    "\n",
    "train_dataset = ImageDataset(t_train_dataset, C)\n",
    "test_dataset = ImageDataset(t_test_dataset, C)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "checkpoint_path = './latest_model.pt'\n",
    "train(model, 30, train_dataset, test_dataset, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))  # load the state of the best model we've seen based on early stopping\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "sample_some(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUT",
   "language": "python",
   "name": "cut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
